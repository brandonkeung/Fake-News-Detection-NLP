{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px9eJ7UC4QbB"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j18qc-Wp4QbB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNnLnwMa4QbB",
        "outputId": "4fb58ee7-d668-47fe-b03f-d43540d15eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yTQ50AK4QbB"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nl5FkZu4QbB"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/liar_dataset/train.tsv\", sep=\"\\t\", header=None, names=\n",
        "                        [\"json_ID\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\", \"state\", \"party\",\n",
        "           \"barely_true\", \"false\", \"half_true\", \"mostly_true\", \"pants_on_fire\", \"source\"])\n",
        "val = pd.read_csv(\"/content/drive/MyDrive/liar_dataset/valid.tsv\", sep=\"\\t\", header=None, names=\n",
        "                        [\"json_ID\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\", \"state\", \"party\",\n",
        "           \"barely_true\", \"false\", \"half_true\", \"mostly_true\", \"pants_on_fire\", \"source\"])\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/liar_dataset/test.tsv\", sep=\"\\t\", header=None, names=\n",
        "                        [\"json_ID\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\", \"state\", \"party\",\n",
        "           \"barely_true\", \"false\", \"half_true\", \"mostly_true\", \"pants_on_fire\", \"source\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "XoRXoakc4QbB",
        "outputId": "bf545576-50e6-4e70-dd66-d96204a4f676"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 10240,\n  \"fields\": [\n    {\n      \"column\": \"json_ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10240,\n        \"samples\": [\n          \"10626.json\",\n          \"1520.json\",\n          \"1326.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"false\",\n          \"half-true\",\n          \"pants-fire\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10223,\n        \"samples\": [\n          \"Countries bombed: Obama 7, Bush 4\",\n          \"Says she couldn't take stimulus money because it required \\\"universal building codes.\\\"\",\n          \"In the past decade, K-12 funding has grown six times faster than the rate of enrollment.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3827,\n        \"samples\": [\n          \"military,new-hampshire-2012\",\n          \"city-budget,city-government,county-budget,county-government,taxes\",\n          \"children,families,poverty,welfare\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speaker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2910,\n        \"samples\": [\n          \"ken-plum\",\n          \"mike-adams\",\n          \"tom-harkin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job_title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1183,\n        \"samples\": [\n          \"Candidate, Multnomah County commission\",\n          \"policy analyst, The Tax Foundation\",\n          \"campaign committee\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 83,\n        \"samples\": [\n          \"Illinois \",\n          \"Texas\",\n          \"Wyoming\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"party\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"tea-party-member\",\n          \"newsmaker\",\n          \"republican\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"barely_true\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.9737643185268,\n        \"min\": 0.0,\n        \"max\": 70.0,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          8.0,\n          13.0,\n          26.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"false\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24.11293628542088,\n        \"min\": 0.0,\n        \"max\": 114.0,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          18.0,\n          43.0,\n          11.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"half_true\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35.846510979322154,\n        \"min\": 0.0,\n        \"max\": 160.0,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          12.0,\n          7.0,\n          38.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mostly_true\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.15168793284596,\n        \"min\": 0.0,\n        \"max\": 163.0,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          4.0,\n          6.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pants_on_fire\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.12892735191374,\n        \"min\": 0.0,\n        \"max\": 105.0,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.0,\n          6.0,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4345,\n        \"samples\": [\n          \"a broadcast of \\\"The Kelly File\\\"\",\n          \"a Spanish-language radio ad\",\n          \"an appearance on ABC's \\\"This Week.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bac336e3-ad6a-4bc8-9716-6019e7197fbd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>json_ID</th>\n",
              "      <th>label</th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>job_title</th>\n",
              "      <th>state</th>\n",
              "      <th>party</th>\n",
              "      <th>barely_true</th>\n",
              "      <th>false</th>\n",
              "      <th>half_true</th>\n",
              "      <th>mostly_true</th>\n",
              "      <th>pants_on_fire</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2635.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a mailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10540.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a floor speech.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>324.json</td>\n",
              "      <td>mostly-true</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>70.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Denver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1123.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health-care</td>\n",
              "      <td>blog-posting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>7.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>a news release</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9028.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>charlie-crist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Florida</td>\n",
              "      <td>democrat</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>an interview on CNN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bac336e3-ad6a-4bc8-9716-6019e7197fbd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bac336e3-ad6a-4bc8-9716-6019e7197fbd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bac336e3-ad6a-4bc8-9716-6019e7197fbd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-17a242b4-e0b7-4cb7-846f-2f4db708e483\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-17a242b4-e0b7-4cb7-846f-2f4db708e483')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-17a242b4-e0b7-4cb7-846f-2f4db708e483 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      json_ID        label                                          statement  \\\n",
              "0   2635.json        false  Says the Annies List political group supports ...   \n",
              "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
              "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
              "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
              "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
              "\n",
              "                              subject         speaker             job_title  \\\n",
              "0                            abortion    dwayne-bohac  State representative   \n",
              "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
              "2                      foreign-policy    barack-obama             President   \n",
              "3                         health-care    blog-posting                   NaN   \n",
              "4                        economy,jobs   charlie-crist                   NaN   \n",
              "\n",
              "      state       party  barely_true  false  half_true  mostly_true  \\\n",
              "0     Texas  republican          0.0    1.0        0.0          0.0   \n",
              "1  Virginia    democrat          0.0    0.0        1.0          1.0   \n",
              "2  Illinois    democrat         70.0   71.0      160.0        163.0   \n",
              "3       NaN        none          7.0   19.0        3.0          5.0   \n",
              "4   Florida    democrat         15.0    9.0       20.0         19.0   \n",
              "\n",
              "   pants_on_fire               source  \n",
              "0            0.0             a mailer  \n",
              "1            0.0      a floor speech.  \n",
              "2            9.0               Denver  \n",
              "3           44.0       a news release  \n",
              "4            2.0  an interview on CNN  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Description of the TSV format:\n",
        "\n",
        "# Column 1: the ID of the statement ([ID].json).\n",
        "# Column 2: the label.\n",
        "# Column 3: the statement.\n",
        "# Column 4: the subject(s).\n",
        "# Column 5: the speaker.\n",
        "# Column 6: the speaker's job title.\n",
        "# Column 7: the state info.\n",
        "# Column 8: the party affiliation.\n",
        "# Column 9-13: the total credit history count, including the current statement.\n",
        "# 9: barely true counts.\n",
        "# 10: false counts.\n",
        "# 11: half true counts.\n",
        "# 12: mostly true counts.\n",
        "# 13: pants on fire counts.\n",
        "# Column 14: the context (venue / location of the speech or statement).\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCVvILLe4QbB"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)\n",
        "\n",
        "def preprocess_data(df):\n",
        "    # Tokenize statements\n",
        "    tokens = tokenizer(list(df['statement']), padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "    labels = torch.tensor(df['label'].map({\"pants-fire\": 0, \"false\": 1, \"barely-true\": 2, \"half-true\": 3, \"mostly-true\": 4, \"true\": 5}).values)\n",
        "    return tokens, labels\n",
        "\n",
        "# Example with train set\n",
        "train_tokens, train_labels = preprocess_data(train)\n",
        "val_tokens, val_labels = preprocess_data(val)\n",
        "test_tokens, test_labels = preprocess_data(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux_QPAOW4QbB"
      },
      "outputs": [],
      "source": [
        "# train_tokens['input_ids'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agM2sF5K4QbB"
      },
      "outputs": [],
      "source": [
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, tokens, labels):\n",
        "        self.input_ids = tokens['input_ids']\n",
        "        self.attention_mask = tokens['attention_mask']\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = FakeNewsDataset(train_tokens, train_labels)\n",
        "val_dataset = FakeNewsDataset(val_tokens, val_labels)\n",
        "test_dataset = FakeNewsDataset(test_tokens, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbcdiKul4QbB"
      },
      "outputs": [],
      "source": [
        "# Set batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBganM2W4QbC",
        "outputId": "b60db844-6e66-4cdb-c7e5-139068b6e728"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n"
          ]
        }
      ],
      "source": [
        "fine_tune = True\n",
        "\n",
        "# Load pre-trained BERT for classification\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=6)\n",
        "# for param in model.bert.parameters():\n",
        "#     param.requires_grad = fine_tune\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=6)\n",
        "for param in model.roberta.parameters():\n",
        "    param.requires_grad = fine_tune\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h6igWJAoM09",
        "outputId": "88c440fa-01cc-42fe-ab7c-af3890b66f27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameters (fine_tuning = True): 355365894\n"
          ]
        }
      ],
      "source": [
        "total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total Parameters (fine_tuning = {fine_tune}): {total_parameters}\")\n",
        "# BERT-base\n",
        "# Total Parameters (fine_tune = True): 109486854\n",
        "# Total Parameters (fine_tune = False): 4614\n",
        "\n",
        "# BERT-large\n",
        "# Total Parameters (fine_tuning = True): 335148038\n",
        "\n",
        "# ROBERTA-large\n",
        "# Total Parameters (fine_tuning = True): 355365894"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72j_7rgG4QbC",
        "outputId": "75dc5600-06bb-420c-f149-f3b9f2d206fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Epoch: 1; Loss: 1.7436107397079468\n",
            "Epoch: 1; Loss: 1.7394465208053589\n",
            "Epoch: 1; Loss: 1.809720754623413\n",
            "Epoch: 1; Loss: 1.8493081331253052\n",
            "Epoch: 1; Loss: 1.9003818035125732\n",
            "Epoch: 1; Loss: 1.914628505706787\n",
            "Epoch: 1; Loss: 1.8500354290008545\n",
            "Epoch: 1; Loss: 1.8578991889953613\n",
            "Epoch: 1; Loss: 1.6941049098968506\n",
            "Epoch: 1; Loss: 1.824720859527588\n",
            "Epoch: 1; Loss: 1.7831525802612305\n",
            "Epoch: 1; Loss: 1.8257478475570679\n",
            "Epoch: 1; Loss: 1.7535582780838013\n",
            "Epoch: 1; Loss: 1.769162654876709\n",
            "Epoch: 1; Loss: 1.960245132446289\n",
            "Epoch: 1; Loss: 1.6681666374206543\n",
            "Epoch: 1; Loss: 1.8073978424072266\n",
            "Epoch: 1; Loss: 1.8584266901016235\n",
            "Epoch: 1; Loss: 1.9005889892578125\n",
            "Epoch: 1; Loss: 1.84257173538208\n",
            "Epoch: 1; Loss: 1.7864519357681274\n",
            "Epoch: 1; Loss: 1.7841532230377197\n",
            "Epoch: 1; Loss: 1.6786938905715942\n",
            "Epoch: 1; Loss: 1.8141645193099976\n",
            "Epoch: 1; Loss: 1.8137214183807373\n",
            "Epoch: 1; Loss: 1.7647291421890259\n",
            "Epoch: 1; Loss: 1.8708306550979614\n",
            "Epoch: 1; Loss: 1.8296353816986084\n",
            "Epoch: 1; Loss: 1.6950750350952148\n",
            "Epoch: 1; Loss: 1.722158432006836\n",
            "Epoch: 1; Loss: 1.9064534902572632\n",
            "Epoch: 1; Loss: 1.846428632736206\n",
            "Epoch: 1; Loss: 1.729049801826477\n",
            "Epoch: 1; Loss: 1.9013621807098389\n",
            "Epoch: 1; Loss: 1.7820051908493042\n",
            "Epoch: 1; Loss: 1.7539682388305664\n",
            "Epoch: 1; Loss: 1.7582789659500122\n",
            "Epoch: 1; Loss: 1.553127408027649\n",
            "Epoch: 1; Loss: 1.7450404167175293\n",
            "Epoch: 1; Loss: 1.7733614444732666\n",
            "Epoch: 1; Loss: 1.832208514213562\n",
            "Epoch: 1; Loss: 1.5819332599639893\n",
            "Epoch: 1; Loss: 1.709308385848999\n",
            "Epoch: 1; Loss: 1.8896979093551636\n",
            "Epoch: 1; Loss: 1.670226812362671\n",
            "Epoch: 1; Loss: 1.8283565044403076\n",
            "Epoch: 1; Loss: 1.709635615348816\n",
            "Epoch: 1; Loss: 1.7348682880401611\n",
            "Epoch: 1; Loss: 1.724718689918518\n",
            "Epoch: 1; Loss: 1.7509740591049194\n",
            "Epoch: 1; Loss: 1.7287768125534058\n",
            "Epoch: 1; Loss: 1.8848296403884888\n",
            "Epoch: 1; Loss: 1.6927725076675415\n",
            "Epoch: 1; Loss: 1.7325420379638672\n",
            "Epoch: 1; Loss: 1.5780972242355347\n",
            "Epoch: 1; Loss: 2.0174760818481445\n",
            "Epoch: 1; Loss: 1.611850380897522\n",
            "Epoch: 1; Loss: 1.9849205017089844\n",
            "Epoch: 1; Loss: 1.8395413160324097\n",
            "Epoch: 1; Loss: 1.701639175415039\n",
            "Epoch: 1; Loss: 1.7400649785995483\n",
            "Epoch: 1; Loss: 1.9337037801742554\n",
            "Epoch: 1; Loss: 1.831091046333313\n",
            "Epoch: 1; Loss: 1.8024605512619019\n",
            "Epoch: 1; Loss: 1.841586709022522\n",
            "Epoch: 1; Loss: 1.7580268383026123\n",
            "Epoch: 1; Loss: 1.7565765380859375\n",
            "Epoch: 1; Loss: 1.7869632244110107\n",
            "Epoch: 1; Loss: 1.9292620420455933\n",
            "Epoch: 1; Loss: 1.8555752038955688\n",
            "Epoch: 1; Loss: 1.8299129009246826\n",
            "Epoch: 1; Loss: 1.7321230173110962\n",
            "Epoch: 1; Loss: 1.7361536026000977\n",
            "Epoch: 1; Loss: 1.783903956413269\n",
            "Epoch: 1; Loss: 1.8283188343048096\n",
            "Epoch: 1; Loss: 1.8522260189056396\n",
            "Epoch: 1; Loss: 1.7245630025863647\n",
            "Epoch: 1; Loss: 1.7647007703781128\n",
            "Epoch: 1; Loss: 1.8360825777053833\n",
            "Epoch: 1; Loss: 1.7741926908493042\n",
            "Epoch: 1; Loss: 1.6885143518447876\n",
            "Epoch: 1; Loss: 1.7187227010726929\n",
            "Epoch: 1; Loss: 1.8483093976974487\n",
            "Epoch: 1; Loss: 1.7635178565979004\n",
            "Epoch: 1; Loss: 1.7509044408798218\n",
            "Epoch: 1; Loss: 1.8460091352462769\n",
            "Epoch: 1; Loss: 1.8139064311981201\n",
            "Epoch: 1; Loss: 1.6862223148345947\n",
            "Epoch: 1; Loss: 1.7639808654785156\n",
            "Epoch: 1; Loss: 1.758225440979004\n",
            "Epoch: 1; Loss: 1.8421094417572021\n",
            "Epoch: 1; Loss: 1.7618907690048218\n",
            "Epoch: 1; Loss: 1.7637258768081665\n",
            "Epoch: 1; Loss: 1.7633509635925293\n",
            "Epoch: 1; Loss: 1.857079267501831\n",
            "Epoch: 1; Loss: 1.9288980960845947\n",
            "Epoch: 1; Loss: 1.8516978025436401\n",
            "Epoch: 1; Loss: 1.7777061462402344\n",
            "Epoch: 1; Loss: 1.7583107948303223\n",
            "Epoch: 1; Loss: 1.7835464477539062\n",
            "Epoch: 1; Loss: 1.8371200561523438\n",
            "Epoch: 1; Loss: 1.8336124420166016\n",
            "Epoch: 1; Loss: 1.8102368116378784\n",
            "Epoch: 1; Loss: 1.8474442958831787\n",
            "Epoch: 1; Loss: 1.776961088180542\n",
            "Epoch: 1; Loss: 1.8147814273834229\n",
            "Epoch: 1; Loss: 1.7913622856140137\n",
            "Epoch: 1; Loss: 1.7281646728515625\n",
            "Epoch: 1; Loss: 1.7523283958435059\n",
            "Epoch: 1; Loss: 1.7674317359924316\n",
            "Epoch: 1; Loss: 1.6945617198944092\n",
            "Epoch: 1; Loss: 1.7689776420593262\n",
            "Epoch: 1; Loss: 1.826581597328186\n",
            "Epoch: 1; Loss: 1.7613909244537354\n",
            "Epoch: 1; Loss: 1.7784113883972168\n",
            "Epoch: 1; Loss: 1.811179518699646\n",
            "Epoch: 1; Loss: 1.8485569953918457\n",
            "Epoch: 1; Loss: 1.7115318775177002\n",
            "Epoch: 1; Loss: 1.8319603204727173\n",
            "Epoch: 1; Loss: 1.8149919509887695\n",
            "Epoch: 1; Loss: 1.7342711687088013\n",
            "Epoch: 1; Loss: 1.7550156116485596\n",
            "Epoch: 1; Loss: 1.7235404253005981\n",
            "Epoch: 1; Loss: 1.8016600608825684\n",
            "Epoch: 1; Loss: 1.767466425895691\n",
            "Epoch: 1; Loss: 1.740167498588562\n",
            "Epoch: 1; Loss: 1.9380263090133667\n",
            "Epoch: 1; Loss: 1.7999361753463745\n",
            "Epoch: 1; Loss: 1.6475555896759033\n",
            "Epoch: 1; Loss: 1.707443118095398\n",
            "Epoch: 1; Loss: 1.753920555114746\n",
            "Epoch: 1; Loss: 1.6333411931991577\n",
            "Epoch: 1; Loss: 1.7781645059585571\n",
            "Epoch: 1; Loss: 1.7012749910354614\n",
            "Epoch: 1; Loss: 1.8082671165466309\n",
            "Epoch: 1; Loss: 1.6665925979614258\n",
            "Epoch: 1; Loss: 1.8291521072387695\n",
            "Epoch: 1; Loss: 1.6647204160690308\n",
            "Epoch: 1; Loss: 1.8596426248550415\n",
            "Epoch: 1; Loss: 1.695769190788269\n",
            "Epoch: 1; Loss: 1.9161204099655151\n",
            "Epoch: 1; Loss: 1.7832385301589966\n",
            "Epoch: 1; Loss: 1.8465112447738647\n",
            "Epoch: 1; Loss: 1.6787455081939697\n",
            "Epoch: 1; Loss: 1.795367956161499\n",
            "Epoch: 1; Loss: 1.7480618953704834\n",
            "Epoch: 1; Loss: 1.7760469913482666\n",
            "Epoch: 1; Loss: 1.8369569778442383\n",
            "Epoch: 1; Loss: 1.7772270441055298\n",
            "Epoch: 1; Loss: 1.9335511922836304\n",
            "Epoch: 1; Loss: 1.7964774370193481\n",
            "Epoch: 1; Loss: 1.865435004234314\n",
            "Epoch: 1; Loss: 1.878647804260254\n",
            "Epoch: 1; Loss: 1.7685632705688477\n",
            "Epoch: 1; Loss: 1.8418831825256348\n",
            "Epoch: 1; Loss: 1.822969675064087\n",
            "Epoch: 1; Loss: 1.7286224365234375\n",
            "Epoch: 1; Loss: 1.770693302154541\n",
            "Epoch: 1; Loss: 1.7616499662399292\n",
            "Epoch: 1; Loss: 1.8081432580947876\n",
            "Epoch: 1; Loss: 1.7377961874008179\n",
            "Epoch: 1; Loss: 1.7114183902740479\n",
            "Epoch: 1; Loss: 1.7616220712661743\n",
            "Epoch: 1; Loss: 1.767866849899292\n",
            "Epoch: 1; Loss: 1.7577786445617676\n",
            "Epoch: 1; Loss: 1.6682045459747314\n",
            "Epoch: 1; Loss: 1.8206698894500732\n",
            "Epoch: 1; Loss: 1.7395203113555908\n",
            "Epoch: 1; Loss: 1.851313829421997\n",
            "Epoch: 1; Loss: 1.7523750066757202\n",
            "Epoch: 1; Loss: 1.8978310823440552\n",
            "Epoch: 1; Loss: 1.7845430374145508\n",
            "Epoch: 1; Loss: 1.8150904178619385\n",
            "Epoch: 1; Loss: 1.9924360513687134\n",
            "Epoch: 1; Loss: 1.6554455757141113\n",
            "Epoch: 1; Loss: 1.775200366973877\n",
            "Epoch: 1; Loss: 1.7858219146728516\n",
            "Epoch: 1; Loss: 1.8418339490890503\n",
            "Epoch: 1; Loss: 1.8048526048660278\n",
            "Epoch: 1; Loss: 1.813293695449829\n",
            "Epoch: 1; Loss: 1.7999531030654907\n",
            "Epoch: 1; Loss: 1.7683818340301514\n",
            "Epoch: 1; Loss: 1.7650550603866577\n",
            "Epoch: 1; Loss: 1.7313880920410156\n",
            "Epoch: 1; Loss: 1.7772440910339355\n",
            "Epoch: 1; Loss: 1.821889042854309\n",
            "Epoch: 1; Loss: 1.7420730590820312\n",
            "Epoch: 1; Loss: 1.7397762537002563\n",
            "Epoch: 1; Loss: 1.7861590385437012\n",
            "Epoch: 1; Loss: 1.7318744659423828\n",
            "Epoch: 1; Loss: 1.8053772449493408\n",
            "Epoch: 1; Loss: 1.719305396080017\n",
            "Epoch: 1; Loss: 1.8027757406234741\n",
            "Epoch: 1; Loss: 1.813830018043518\n",
            "Epoch: 1; Loss: 1.7670055627822876\n",
            "Epoch: 1; Loss: 1.7953521013259888\n",
            "Epoch: 1; Loss: 1.8061374425888062\n",
            "Epoch: 1; Loss: 1.8554322719573975\n",
            "Epoch: 1; Loss: 1.7398123741149902\n",
            "Epoch: 1; Loss: 1.8197236061096191\n",
            "Epoch: 1; Loss: 1.7619547843933105\n",
            "Epoch: 1; Loss: 1.8561885356903076\n",
            "Epoch: 1; Loss: 1.739478349685669\n",
            "Epoch: 1; Loss: 1.8188927173614502\n",
            "Epoch: 1; Loss: 1.633293867111206\n",
            "Epoch: 1; Loss: 1.816145658493042\n",
            "Epoch: 1; Loss: 1.8122050762176514\n",
            "Epoch: 1; Loss: 1.7181005477905273\n",
            "Epoch: 1; Loss: 1.9153029918670654\n",
            "Epoch: 1; Loss: 1.7442207336425781\n",
            "Epoch: 1; Loss: 1.7923861742019653\n",
            "Epoch: 1; Loss: 1.708128571510315\n",
            "Epoch: 1; Loss: 1.7610864639282227\n",
            "Epoch: 1; Loss: 1.7417913675308228\n",
            "Epoch: 1; Loss: 1.7022167444229126\n",
            "Epoch: 1; Loss: 1.7153323888778687\n",
            "Epoch: 1; Loss: 1.7384268045425415\n",
            "Epoch: 1; Loss: 1.7904787063598633\n",
            "Epoch: 1; Loss: 1.693278193473816\n",
            "Epoch: 1; Loss: 1.7832430601119995\n",
            "Epoch: 1; Loss: 1.6642519235610962\n",
            "Epoch: 1; Loss: 1.8320595026016235\n",
            "Epoch: 1; Loss: 1.9880651235580444\n",
            "Epoch: 1; Loss: 1.7633311748504639\n",
            "Epoch: 1; Loss: 1.7166739702224731\n",
            "Epoch: 1; Loss: 1.6471607685089111\n",
            "Epoch: 1; Loss: 1.6954519748687744\n",
            "Epoch: 1; Loss: 2.000725746154785\n",
            "Epoch: 1; Loss: 1.779335618019104\n",
            "Epoch: 1; Loss: 1.7388006448745728\n",
            "Epoch: 1; Loss: 1.705918788909912\n",
            "Epoch: 1; Loss: 1.8161342144012451\n",
            "Epoch: 1; Loss: 1.7810527086257935\n",
            "Epoch: 1; Loss: 1.9044606685638428\n",
            "Epoch: 1; Loss: 1.7002179622650146\n",
            "Epoch: 1; Loss: 1.7387577295303345\n",
            "Epoch: 1; Loss: 1.8498451709747314\n",
            "Epoch: 1; Loss: 1.7814244031906128\n",
            "Epoch: 1; Loss: 1.7857216596603394\n",
            "Epoch: 1; Loss: 1.7688992023468018\n",
            "Epoch: 1; Loss: 1.7679002285003662\n",
            "Epoch: 1; Loss: 1.8194535970687866\n",
            "Epoch: 1; Loss: 1.725745439529419\n",
            "Epoch: 1; Loss: 1.7559561729431152\n",
            "Epoch: 1; Loss: 1.737471342086792\n",
            "Epoch: 1; Loss: 1.794687271118164\n",
            "Epoch: 1; Loss: 1.7333991527557373\n",
            "Epoch: 1; Loss: 1.6996064186096191\n",
            "Epoch: 1; Loss: 1.718191146850586\n",
            "Epoch: 1; Loss: 1.8376433849334717\n",
            "Epoch: 1; Loss: 1.8478989601135254\n",
            "Epoch: 1; Loss: 1.790112018585205\n",
            "Epoch: 1; Loss: 1.7546969652175903\n",
            "Epoch: 1; Loss: 1.7197480201721191\n",
            "Epoch: 1; Loss: 1.6371595859527588\n",
            "Epoch: 1; Loss: 1.76263427734375\n",
            "Epoch: 1; Loss: 1.8054313659667969\n",
            "Epoch: 1; Loss: 1.8319804668426514\n",
            "Epoch: 1; Loss: 1.786596655845642\n",
            "Epoch: 1; Loss: 1.8909034729003906\n",
            "Epoch: 1; Loss: 1.7427977323532104\n",
            "Epoch: 1; Loss: 1.7543647289276123\n",
            "Epoch: 1; Loss: 1.7022922039031982\n",
            "Epoch: 1; Loss: 1.6510064601898193\n",
            "Epoch: 1; Loss: 1.75589120388031\n",
            "Epoch: 1; Loss: 1.7882086038589478\n",
            "Epoch: 1; Loss: 1.7550346851348877\n",
            "Epoch: 1; Loss: 1.7237588167190552\n",
            "Epoch: 1; Loss: 1.7137542963027954\n",
            "Epoch: 1; Loss: 1.8386077880859375\n",
            "Epoch: 1; Loss: 1.733798623085022\n",
            "Epoch: 1; Loss: 1.8558990955352783\n",
            "Epoch: 1; Loss: 1.8886654376983643\n",
            "Epoch: 1; Loss: 1.6674731969833374\n",
            "Epoch: 1; Loss: 1.8618712425231934\n",
            "Epoch: 1; Loss: 1.82810378074646\n",
            "Epoch: 1; Loss: 1.7914246320724487\n",
            "Epoch: 1; Loss: 1.7384374141693115\n",
            "Epoch: 1; Loss: 1.8704415559768677\n",
            "Epoch: 1; Loss: 1.713980793952942\n",
            "Epoch: 1; Loss: 1.7808740139007568\n",
            "Epoch: 1; Loss: 1.6502480506896973\n",
            "Epoch: 1; Loss: 1.7580780982971191\n",
            "Epoch: 1; Loss: 1.7121511697769165\n",
            "Epoch: 1; Loss: 1.7315921783447266\n",
            "Epoch: 1; Loss: 1.7661471366882324\n",
            "Epoch: 1; Loss: 1.7375481128692627\n",
            "Epoch: 1; Loss: 1.7478289604187012\n",
            "Epoch: 1; Loss: 1.8488976955413818\n",
            "Epoch: 1; Loss: 1.7751047611236572\n",
            "Epoch: 1; Loss: 1.8173719644546509\n",
            "Epoch: 1; Loss: 1.813353180885315\n",
            "Epoch: 1; Loss: 1.7142199277877808\n",
            "Epoch: 1; Loss: 1.7538344860076904\n",
            "Epoch: 1; Loss: 1.701387643814087\n",
            "Epoch: 1; Loss: 1.8348995447158813\n",
            "Epoch: 1; Loss: 1.6398829221725464\n",
            "Epoch: 1; Loss: 1.7686229944229126\n",
            "Epoch: 1; Loss: 1.7451810836791992\n",
            "Epoch: 1; Loss: 1.6962617635726929\n",
            "Epoch: 1; Loss: 1.6715471744537354\n",
            "Epoch: 1; Loss: 1.6925880908966064\n",
            "Epoch: 1; Loss: 1.7900476455688477\n",
            "Epoch: 1; Loss: 1.7061463594436646\n",
            "Epoch: 1; Loss: 1.680515170097351\n",
            "Epoch: 1; Loss: 1.7330018281936646\n",
            "Epoch: 1; Loss: 1.6986926794052124\n",
            "Epoch: 1; Loss: 1.7251818180084229\n",
            "Epoch: 1; Loss: 1.7528769969940186\n",
            "Epoch: 1; Loss: 1.6394366025924683\n",
            "Epoch: 1; Loss: 1.6361199617385864\n",
            "Epoch: 1; Loss: 1.7738672494888306\n",
            "Epoch: 1; Loss: 1.8105038404464722\n",
            "Epoch: 1; Loss: 1.8579211235046387\n",
            "Epoch: 1; Loss: 1.894158124923706\n",
            "Epoch: 1; Loss: 1.6883845329284668\n",
            "Epoch: 1; Loss: 1.9572055339813232\n",
            "Epoch: 1; Loss: 1.7752747535705566\n",
            "Epoch: 1; Loss: 1.8597952127456665\n",
            "Epoch: 1; Loss: 1.8584377765655518\n",
            "Epoch: 1; Loss: 1.7632981538772583\n",
            "Epoch: 1; Loss: 1.791529893875122\n",
            "Epoch: 1; Loss: 1.8171167373657227\n",
            "Epoch: 1; Loss: 1.7730270624160767\n",
            "Epoch: 1; Loss: 1.7063441276550293\n",
            "Epoch: 1; Loss: 1.8189746141433716\n",
            "Epoch: 1; Loss: 1.902099370956421\n",
            "Epoch: 1; Loss: 1.7777560949325562\n",
            "Epoch: 1; Loss: 1.7394074201583862\n",
            "Epoch: 1; Loss: 1.7503501176834106\n",
            "Epoch: 1; Loss: 1.7685973644256592\n",
            "Epoch: 1; Loss: 1.8175766468048096\n",
            "Epoch: 1; Loss: 1.6979520320892334\n",
            "Epoch: 1; Loss: 1.7678656578063965\n",
            "Epoch: 1; Loss: 1.7138714790344238\n",
            "Epoch: 1; Loss: 1.8612239360809326\n",
            "Epoch: 1; Loss: 1.730315923690796\n",
            "Epoch: 1; Loss: 1.7719333171844482\n",
            "Epoch: 1; Loss: 1.7342603206634521\n",
            "Epoch: 1; Loss: 1.885779857635498\n",
            "Epoch: 1; Loss: 1.832943081855774\n",
            "Epoch: 1; Loss: 1.7813090085983276\n",
            "Epoch: 1; Loss: 1.7212787866592407\n",
            "Epoch: 1; Loss: 1.8146092891693115\n",
            "Epoch: 1; Loss: 1.8259146213531494\n",
            "Epoch: 1; Loss: 1.9451591968536377\n",
            "Epoch: 1; Loss: 1.6016634702682495\n",
            "Epoch: 1; Loss: 1.7478083372116089\n",
            "Epoch: 1; Loss: 1.7887054681777954\n",
            "Epoch: 1; Loss: 1.7316937446594238\n",
            "Epoch: 1; Loss: 1.881925106048584\n",
            "Epoch: 1; Loss: 1.828247308731079\n",
            "Epoch: 1; Loss: 1.802646517753601\n",
            "Epoch: 1; Loss: 1.6958364248275757\n",
            "Epoch: 1; Loss: 1.7437536716461182\n",
            "Epoch: 1; Loss: 1.844137191772461\n",
            "Epoch: 1; Loss: 1.6712462902069092\n",
            "Epoch: 1; Loss: 1.6516737937927246\n",
            "Epoch: 1; Loss: 1.694191575050354\n",
            "Epoch: 1; Loss: 1.732943058013916\n",
            "Epoch: 1; Loss: 1.8043862581253052\n",
            "Epoch: 1; Loss: 1.8429858684539795\n",
            "Epoch: 1; Loss: 1.84525465965271\n",
            "Epoch: 1; Loss: 1.793261170387268\n",
            "Epoch: 1; Loss: 1.699851155281067\n",
            "Epoch: 1; Loss: 1.6478033065795898\n",
            "Epoch: 1; Loss: 1.7224199771881104\n",
            "Epoch: 1; Loss: 1.7460886240005493\n",
            "Epoch: 1; Loss: 1.7693920135498047\n",
            "Epoch: 1; Loss: 1.7396043539047241\n",
            "Epoch: 1; Loss: 1.8300364017486572\n",
            "Epoch: 1; Loss: 1.6739211082458496\n",
            "Epoch: 1; Loss: 1.8301656246185303\n",
            "Epoch: 1; Loss: 1.7878761291503906\n",
            "Epoch: 1; Loss: 1.6311700344085693\n",
            "Epoch: 1; Loss: 1.6273436546325684\n",
            "Epoch: 1; Loss: 1.6296303272247314\n",
            "Epoch: 1; Loss: 1.7416985034942627\n",
            "Epoch: 1; Loss: 1.6594171524047852\n",
            "Epoch: 1; Loss: 1.7133002281188965\n",
            "Epoch: 1; Loss: 1.62015962600708\n",
            "Epoch: 1; Loss: 1.6508870124816895\n",
            "Epoch: 1; Loss: 1.9401519298553467\n",
            "Epoch: 1; Loss: 1.71710205078125\n",
            "Epoch: 1; Loss: 1.641602635383606\n",
            "Epoch: 1; Loss: 1.6077193021774292\n",
            "Epoch: 1; Loss: 1.745295763015747\n",
            "Epoch: 1; Loss: 1.720415472984314\n",
            "Epoch: 1; Loss: 1.667881965637207\n",
            "Epoch: 1; Loss: 1.832929253578186\n",
            "Epoch: 1; Loss: 1.6797398328781128\n",
            "Epoch: 1; Loss: 2.0509567260742188\n",
            "Epoch: 1; Loss: 1.7490535974502563\n",
            "Epoch: 1; Loss: 1.732636570930481\n",
            "Epoch: 1; Loss: 1.7752422094345093\n",
            "Epoch: 1; Loss: 1.5831197500228882\n",
            "Epoch: 1; Loss: 1.5981365442276\n",
            "Epoch: 1; Loss: 1.9748013019561768\n",
            "Epoch: 1; Loss: 1.8513140678405762\n",
            "Epoch: 1; Loss: 1.747328519821167\n",
            "Epoch: 1; Loss: 1.66346275806427\n",
            "Epoch: 1; Loss: 1.7147893905639648\n",
            "Epoch: 1; Loss: 1.8039977550506592\n",
            "Epoch: 1; Loss: 1.659393310546875\n",
            "Epoch: 1; Loss: 1.59881591796875\n",
            "Epoch: 1; Loss: 1.6502538919448853\n",
            "Epoch: 1; Loss: 2.007031202316284\n",
            "Epoch: 1; Loss: 1.8305437564849854\n",
            "Epoch: 1; Loss: 1.6386927366256714\n",
            "Epoch: 1; Loss: 1.7861193418502808\n",
            "Epoch: 1; Loss: 1.6983810663223267\n",
            "Epoch: 1; Loss: 1.6918479204177856\n",
            "Epoch: 1; Loss: 1.6531180143356323\n",
            "Epoch: 1; Loss: 1.7652195692062378\n",
            "Epoch: 1; Loss: 1.6142300367355347\n",
            "Epoch: 1; Loss: 1.8108972311019897\n",
            "Epoch: 1; Loss: 1.6401020288467407\n",
            "Epoch: 1; Loss: 1.801600456237793\n",
            "Epoch: 1; Loss: 1.8441799879074097\n",
            "Epoch: 1; Loss: 1.7699415683746338\n",
            "Epoch: 1; Loss: 1.6943063735961914\n",
            "Epoch: 1; Loss: 1.925768256187439\n",
            "Epoch: 1; Loss: 1.779309868812561\n",
            "Epoch: 1; Loss: 1.701266884803772\n",
            "Epoch: 1; Loss: 1.6639974117279053\n",
            "Epoch: 1; Loss: 1.8324460983276367\n",
            "Epoch: 1; Loss: 1.7265907526016235\n",
            "Epoch: 1; Loss: 1.7528458833694458\n",
            "Epoch: 1; Loss: 1.8110779523849487\n",
            "Epoch: 1; Loss: 1.6870088577270508\n",
            "Epoch: 1; Loss: 1.7233271598815918\n",
            "Epoch: 1; Loss: 1.739851713180542\n",
            "Epoch: 1; Loss: 1.6805450916290283\n",
            "Epoch: 1; Loss: 1.8591928482055664\n",
            "Epoch: 1; Loss: 1.7786277532577515\n",
            "Epoch: 1; Loss: 1.9331953525543213\n",
            "Epoch: 1; Loss: 1.6830189228057861\n",
            "Epoch: 1; Loss: 1.8521367311477661\n",
            "Epoch: 1; Loss: 1.7459050416946411\n",
            "Epoch: 1; Loss: 1.7575654983520508\n",
            "Epoch: 1; Loss: 1.5578866004943848\n",
            "Epoch: 1; Loss: 1.685839056968689\n",
            "Epoch: 1; Loss: 1.8988101482391357\n",
            "Epoch: 1; Loss: 1.8183281421661377\n",
            "Epoch: 1; Loss: 1.7081199884414673\n",
            "Epoch: 1; Loss: 1.7177892923355103\n",
            "Epoch: 1; Loss: 1.8203868865966797\n",
            "Epoch: 1; Loss: 1.7781810760498047\n",
            "Epoch: 1; Loss: 1.7114824056625366\n",
            "Epoch: 1; Loss: 1.7064909934997559\n",
            "Epoch: 1; Loss: 1.7115328311920166\n",
            "Epoch: 1; Loss: 1.6667637825012207\n",
            "Epoch: 1; Loss: 1.7826552391052246\n",
            "Epoch: 1; Loss: 1.8045603036880493\n",
            "Epoch: 1; Loss: 1.7725584506988525\n",
            "Epoch: 1; Loss: 1.7441726922988892\n",
            "Epoch: 1; Loss: 1.730700969696045\n",
            "Epoch: 1; Loss: 1.715600609779358\n",
            "Epoch: 1; Loss: 1.8155988454818726\n",
            "Epoch: 1; Loss: 1.6077386140823364\n",
            "Epoch: 1; Loss: 1.653269648551941\n",
            "Epoch: 1; Loss: 1.8263802528381348\n",
            "Epoch: 1; Loss: 1.722686767578125\n",
            "Epoch: 1; Loss: 1.7280524969100952\n",
            "Epoch: 1; Loss: 1.7419800758361816\n",
            "Epoch: 1; Loss: 1.7988462448120117\n",
            "Epoch: 1; Loss: 1.697381854057312\n",
            "Epoch: 1; Loss: 1.7721997499465942\n",
            "Epoch: 1; Loss: 1.9923970699310303\n",
            "Epoch: 1; Loss: 1.7518805265426636\n",
            "Epoch: 1; Loss: 1.7074263095855713\n",
            "Epoch: 1; Loss: 1.740049958229065\n",
            "Epoch: 1; Loss: 1.7285757064819336\n",
            "Epoch: 1; Loss: 1.6622027158737183\n",
            "Epoch: 1; Loss: 1.7782448530197144\n",
            "Epoch: 1; Loss: 1.7551597356796265\n",
            "Epoch: 1; Loss: 1.8716659545898438\n",
            "Epoch: 1; Loss: 1.7124613523483276\n",
            "Epoch: 1; Loss: 1.8637455701828003\n",
            "Epoch: 1; Loss: 1.652966856956482\n",
            "Epoch: 1; Loss: 1.7523036003112793\n",
            "Epoch: 1; Loss: 1.7953922748565674\n",
            "Epoch: 1; Loss: 1.7570222616195679\n",
            "Epoch: 1; Loss: 1.7727272510528564\n",
            "Epoch: 1; Loss: 1.844280481338501\n",
            "Epoch: 1; Loss: 1.6610469818115234\n",
            "Epoch: 1; Loss: 1.6908137798309326\n",
            "Epoch: 1; Loss: 1.8784456253051758\n",
            "Epoch: 1; Loss: 1.6648203134536743\n",
            "Epoch: 1; Loss: 1.7147367000579834\n",
            "Epoch: 1; Loss: 1.6894543170928955\n",
            "Epoch: 1; Loss: 1.698506474494934\n",
            "Epoch: 1; Loss: 1.6803867816925049\n",
            "Epoch: 1; Loss: 1.789049506187439\n",
            "Epoch: 1; Loss: 1.6247624158859253\n",
            "Epoch: 1; Loss: 1.7606465816497803\n",
            "Epoch: 1; Loss: 1.8043851852416992\n",
            "Epoch: 1; Loss: 1.5871134996414185\n",
            "Epoch: 1; Loss: 1.6819956302642822\n",
            "Epoch: 1; Loss: 1.5869348049163818\n",
            "Epoch: 1; Loss: 1.586495280265808\n",
            "Epoch: 1; Loss: 1.7276875972747803\n",
            "Epoch: 1; Loss: 1.6446011066436768\n",
            "Epoch: 1; Loss: 1.64628005027771\n",
            "Epoch: 1; Loss: 1.600243091583252\n",
            "Epoch: 1; Loss: 1.7569849491119385\n",
            "Epoch: 1; Loss: 1.5149149894714355\n",
            "Epoch: 1; Loss: 1.921720027923584\n",
            "Epoch: 1; Loss: 1.8066086769104004\n",
            "Epoch: 1; Loss: 1.7571995258331299\n",
            "Epoch: 1; Loss: 1.5212889909744263\n",
            "Epoch: 1; Loss: 1.8970190286636353\n",
            "Epoch: 1; Loss: 1.9343067407608032\n",
            "Epoch: 1; Loss: 1.583353042602539\n",
            "Epoch: 1; Loss: 1.6841390132904053\n",
            "Epoch: 1; Loss: 1.6361597776412964\n",
            "Epoch: 1; Loss: 1.8101569414138794\n",
            "Epoch: 1; Loss: 1.7918668985366821\n",
            "Epoch: 1; Loss: 1.7347627878189087\n",
            "Epoch: 1; Loss: 1.8510644435882568\n",
            "Epoch: 1; Loss: 1.7132707834243774\n",
            "Epoch: 1; Loss: 1.7211962938308716\n",
            "Epoch: 1; Loss: 1.6843187808990479\n",
            "Epoch: 1; Loss: 1.745833396911621\n",
            "Epoch: 1; Loss: 1.7997007369995117\n",
            "Epoch: 1; Loss: 1.8501336574554443\n",
            "Epoch: 1; Loss: 1.7472113370895386\n",
            "Epoch: 1; Loss: 1.7188384532928467\n",
            "Epoch: 1; Loss: 1.7330156564712524\n",
            "Epoch: 1; Loss: 1.7107406854629517\n",
            "Epoch: 1; Loss: 1.7739331722259521\n",
            "Epoch: 1; Loss: 1.7218201160430908\n",
            "Epoch: 1; Loss: 1.7701754570007324\n",
            "Epoch: 1; Loss: 1.6938128471374512\n",
            "Epoch: 1; Loss: 1.7398761510849\n",
            "Epoch: 1; Loss: 1.8035311698913574\n",
            "Epoch: 1; Loss: 1.6315995454788208\n",
            "Epoch: 1; Loss: 1.7217411994934082\n",
            "Epoch: 1; Loss: 1.7629051208496094\n",
            "Epoch: 1; Loss: 1.7529836893081665\n",
            "Epoch: 1; Loss: 1.6777373552322388\n",
            "Epoch: 1; Loss: 1.779079556465149\n",
            "Epoch: 1; Loss: 1.6998615264892578\n",
            "Epoch: 1; Loss: 1.7605140209197998\n",
            "Epoch: 1; Loss: 1.7618671655654907\n",
            "Epoch: 1; Loss: 1.6926876306533813\n",
            "Epoch: 1; Loss: 1.6534972190856934\n",
            "Epoch: 1; Loss: 1.6730481386184692\n",
            "Epoch: 1; Loss: 1.838025689125061\n",
            "Epoch: 1; Loss: 1.6584974527359009\n",
            "Epoch: 1; Loss: 1.75294029712677\n",
            "Epoch: 1; Loss: 1.6238893270492554\n",
            "Epoch: 1; Loss: 1.6887074708938599\n",
            "Epoch: 1; Loss: 1.6706275939941406\n",
            "Epoch: 1; Loss: 1.7277252674102783\n",
            "Epoch: 1; Loss: 1.6948026418685913\n",
            "Epoch: 1; Loss: 1.8903270959854126\n",
            "Epoch: 1; Loss: 1.7867730855941772\n",
            "Epoch: 1; Loss: 1.7171880006790161\n",
            "Epoch: 1; Loss: 1.8016364574432373\n",
            "Epoch: 1; Loss: 1.7292592525482178\n",
            "Epoch: 1; Loss: 1.6994378566741943\n",
            "Epoch: 1; Loss: 1.6528363227844238\n",
            "Epoch: 1; Loss: 1.62461519241333\n",
            "Epoch: 1; Loss: 1.8076951503753662\n",
            "Epoch: 1; Loss: 1.7129117250442505\n",
            "Epoch: 1; Loss: 1.6536201238632202\n",
            "Epoch: 1; Loss: 1.933974266052246\n",
            "Epoch: 1; Loss: 1.8008835315704346\n",
            "Epoch: 1; Loss: 1.6961466073989868\n",
            "Epoch: 1; Loss: 1.6547377109527588\n",
            "Epoch: 1; Loss: 1.6512408256530762\n",
            "Epoch: 1; Loss: 1.7108206748962402\n",
            "Epoch: 1; Loss: 1.8678539991378784\n",
            "Epoch: 1; Loss: 1.7192893028259277\n",
            "Epoch: 1; Loss: 1.6336758136749268\n",
            "Epoch: 1; Loss: 1.5320699214935303\n",
            "Epoch: 1; Loss: 1.678283929824829\n",
            "Epoch: 1; Loss: 1.8578695058822632\n",
            "Epoch: 1; Loss: 1.5926234722137451\n",
            "Epoch: 1; Loss: 1.7386573553085327\n",
            "Epoch: 1; Loss: 1.8066459894180298\n",
            "Epoch: 1; Loss: 1.6203138828277588\n",
            "Epoch: 1; Loss: 1.780458927154541\n",
            "Epoch: 1; Loss: 1.6943395137786865\n",
            "Epoch: 1; Loss: 1.7958769798278809\n",
            "Epoch: 1; Loss: 1.7701746225357056\n",
            "Epoch: 1; Loss: 1.6634975671768188\n",
            "Epoch: 1; Loss: 1.8091802597045898\n",
            "Epoch: 1; Loss: 1.594958782196045\n",
            "Epoch: 1; Loss: 1.8531938791275024\n",
            "Epoch: 1; Loss: 1.6719101667404175\n",
            "Epoch: 1; Loss: 1.5116506814956665\n",
            "Epoch: 1; Loss: 1.6043835878372192\n",
            "Epoch: 1; Loss: 1.9056192636489868\n",
            "Epoch: 1; Loss: 1.7636851072311401\n",
            "Epoch: 1; Loss: 1.6319175958633423\n",
            "Epoch: 1; Loss: 1.5754458904266357\n",
            "Epoch: 1; Loss: 1.7722233533859253\n",
            "Epoch: 1; Loss: 1.7562952041625977\n",
            "Epoch: 1; Loss: 1.6851977109909058\n",
            "Epoch: 1; Loss: 1.8860007524490356\n",
            "Epoch: 1; Loss: 1.5779392719268799\n",
            "Epoch: 1; Loss: 1.712396264076233\n",
            "Epoch: 1; Loss: 1.6631226539611816\n",
            "Epoch: 1; Loss: 1.6476998329162598\n",
            "Epoch: 1; Loss: 1.7851747274398804\n",
            "Epoch: 1; Loss: 1.5679327249526978\n",
            "Epoch: 1; Loss: 1.758765459060669\n",
            "Epoch: 1; Loss: 1.7176945209503174\n",
            "Epoch: 1; Loss: 1.9369571208953857\n",
            "Epoch: 1; Loss: 1.588063359260559\n",
            "Epoch: 1; Loss: 1.695556402206421\n",
            "Epoch: 1; Loss: 1.742841124534607\n",
            "Epoch: 1; Loss: 1.610221266746521\n",
            "Epoch: 1; Loss: 1.640534520149231\n",
            "Epoch: 1; Loss: 1.6866761445999146\n",
            "Epoch: 1; Loss: 1.667959213256836\n",
            "Epoch: 1; Loss: 1.7743299007415771\n",
            "Epoch: 1; Loss: 1.6194233894348145\n",
            "Epoch: 1; Loss: 1.8214056491851807\n",
            "Epoch: 1; Loss: 1.5045925378799438\n",
            "Epoch: 1; Loss: 1.683995246887207\n",
            "Epoch: 1; Loss: 1.6949079036712646\n",
            "Epoch: 1; Loss: 1.7878798246383667\n",
            "Epoch: 1; Loss: 1.7010533809661865\n",
            "Epoch: 1; Loss: 1.6165506839752197\n",
            "Epoch: 1; Loss: 1.6118435859680176\n",
            "Epoch: 1; Loss: 1.792973518371582\n",
            "Epoch: 1; Loss: 1.7663451433181763\n",
            "Epoch: 1; Loss: 1.767482042312622\n",
            "Epoch: 1; Loss: 1.7761144638061523\n",
            "Epoch: 1; Loss: 1.6793925762176514\n",
            "Epoch: 1; Loss: 1.5959964990615845\n",
            "Epoch: 1; Loss: 1.814272403717041\n",
            "Epoch: 1; Loss: 1.6251755952835083\n",
            "Epoch: 1; Loss: 1.724663496017456\n",
            "Epoch: 1; Loss: 1.5878642797470093\n",
            "Epoch: 1; Loss: 1.5543190240859985\n",
            "Epoch: 1; Loss: 1.7752642631530762\n",
            "Epoch 2/5\n",
            "Epoch: 2; Loss: 1.6821478605270386\n",
            "Epoch: 2; Loss: 1.8280149698257446\n",
            "Epoch: 2; Loss: 1.6602997779846191\n",
            "Epoch: 2; Loss: 1.7949498891830444\n",
            "Epoch: 2; Loss: 1.5889908075332642\n",
            "Epoch: 2; Loss: 1.713341474533081\n",
            "Epoch: 2; Loss: 1.6088600158691406\n",
            "Epoch: 2; Loss: 1.6437031030654907\n",
            "Epoch: 2; Loss: 1.7420811653137207\n",
            "Epoch: 2; Loss: 1.8345246315002441\n",
            "Epoch: 2; Loss: 1.6816092729568481\n",
            "Epoch: 2; Loss: 1.6656298637390137\n",
            "Epoch: 2; Loss: 1.753948450088501\n",
            "Epoch: 2; Loss: 1.701252818107605\n",
            "Epoch: 2; Loss: 1.5892680883407593\n",
            "Epoch: 2; Loss: 1.7988823652267456\n",
            "Epoch: 2; Loss: 1.766586422920227\n",
            "Epoch: 2; Loss: 1.5755257606506348\n",
            "Epoch: 2; Loss: 1.7027353048324585\n",
            "Epoch: 2; Loss: 1.848312258720398\n",
            "Epoch: 2; Loss: 1.6510202884674072\n",
            "Epoch: 2; Loss: 1.580051302909851\n",
            "Epoch: 2; Loss: 1.7378660440444946\n",
            "Epoch: 2; Loss: 1.7575738430023193\n",
            "Epoch: 2; Loss: 1.8353610038757324\n",
            "Epoch: 2; Loss: 1.6648181676864624\n",
            "Epoch: 2; Loss: 1.7675541639328003\n",
            "Epoch: 2; Loss: 1.7067228555679321\n",
            "Epoch: 2; Loss: 1.7297346591949463\n",
            "Epoch: 2; Loss: 1.6351516246795654\n",
            "Epoch: 2; Loss: 1.6209810972213745\n",
            "Epoch: 2; Loss: 1.7723885774612427\n",
            "Epoch: 2; Loss: 1.6818311214447021\n",
            "Epoch: 2; Loss: 1.6300742626190186\n",
            "Epoch: 2; Loss: 1.8694241046905518\n",
            "Epoch: 2; Loss: 1.6765868663787842\n",
            "Epoch: 2; Loss: 1.6904923915863037\n",
            "Epoch: 2; Loss: 1.7179112434387207\n",
            "Epoch: 2; Loss: 1.5862278938293457\n",
            "Epoch: 2; Loss: 1.8862719535827637\n",
            "Epoch: 2; Loss: 1.7394115924835205\n",
            "Epoch: 2; Loss: 1.7557212114334106\n",
            "Epoch: 2; Loss: 1.7681995630264282\n",
            "Epoch: 2; Loss: 1.733848214149475\n",
            "Epoch: 2; Loss: 1.6963669061660767\n",
            "Epoch: 2; Loss: 1.8480969667434692\n",
            "Epoch: 2; Loss: 1.6166623830795288\n",
            "Epoch: 2; Loss: 1.588699460029602\n",
            "Epoch: 2; Loss: 1.6468082666397095\n",
            "Epoch: 2; Loss: 1.4898072481155396\n",
            "Epoch: 2; Loss: 1.5574296712875366\n",
            "Epoch: 2; Loss: 1.76104736328125\n",
            "Epoch: 2; Loss: 1.7444218397140503\n",
            "Epoch: 2; Loss: 1.922471284866333\n",
            "Epoch: 2; Loss: 1.6340373754501343\n",
            "Epoch: 2; Loss: 1.57794189453125\n",
            "Epoch: 2; Loss: 2.0433127880096436\n",
            "Epoch: 2; Loss: 1.8247877359390259\n",
            "Epoch: 2; Loss: 2.0739312171936035\n",
            "Epoch: 2; Loss: 1.6115294694900513\n",
            "Epoch: 2; Loss: 1.6807187795639038\n",
            "Epoch: 2; Loss: 1.8285082578659058\n",
            "Epoch: 2; Loss: 1.8850106000900269\n",
            "Epoch: 2; Loss: 1.7838027477264404\n",
            "Epoch: 2; Loss: 1.6984360218048096\n",
            "Epoch: 2; Loss: 1.7467721700668335\n",
            "Epoch: 2; Loss: 1.687367558479309\n",
            "Epoch: 2; Loss: 1.7429296970367432\n",
            "Epoch: 2; Loss: 1.7950708866119385\n",
            "Epoch: 2; Loss: 1.7848533391952515\n",
            "Epoch: 2; Loss: 1.839079737663269\n",
            "Epoch: 2; Loss: 1.7515392303466797\n",
            "Epoch: 2; Loss: 1.7479265928268433\n",
            "Epoch: 2; Loss: 1.7499666213989258\n",
            "Epoch: 2; Loss: 1.7108519077301025\n",
            "Epoch: 2; Loss: 1.673561692237854\n",
            "Epoch: 2; Loss: 1.6637749671936035\n",
            "Epoch: 2; Loss: 1.5484620332717896\n",
            "Epoch: 2; Loss: 1.6378235816955566\n",
            "Epoch: 2; Loss: 1.8298813104629517\n",
            "Epoch: 2; Loss: 1.8006694316864014\n",
            "Epoch: 2; Loss: 1.6554449796676636\n",
            "Epoch: 2; Loss: 1.846047282218933\n",
            "Epoch: 2; Loss: 1.8177067041397095\n",
            "Epoch: 2; Loss: 1.8347607851028442\n",
            "Epoch: 2; Loss: 1.648087978363037\n",
            "Epoch: 2; Loss: 1.6781028509140015\n",
            "Epoch: 2; Loss: 1.635036826133728\n",
            "Epoch: 2; Loss: 1.6833090782165527\n",
            "Epoch: 2; Loss: 1.819907546043396\n",
            "Epoch: 2; Loss: 1.5628482103347778\n",
            "Epoch: 2; Loss: 1.7064123153686523\n",
            "Epoch: 2; Loss: 1.7342970371246338\n",
            "Epoch: 2; Loss: 1.839227557182312\n",
            "Epoch: 2; Loss: 1.6615468263626099\n",
            "Epoch: 2; Loss: 1.8424066305160522\n",
            "Epoch: 2; Loss: 1.6981991529464722\n",
            "Epoch: 2; Loss: 1.704905390739441\n",
            "Epoch: 2; Loss: 1.7103204727172852\n",
            "Epoch: 2; Loss: 1.696895718574524\n",
            "Epoch: 2; Loss: 1.5629123449325562\n",
            "Epoch: 2; Loss: 1.678066611289978\n",
            "Epoch: 2; Loss: 1.5403132438659668\n",
            "Epoch: 2; Loss: 1.5018688440322876\n",
            "Epoch: 2; Loss: 1.6388492584228516\n",
            "Epoch: 2; Loss: 1.6011193990707397\n",
            "Epoch: 2; Loss: 1.7233023643493652\n",
            "Epoch: 2; Loss: 1.5653245449066162\n",
            "Epoch: 2; Loss: 1.833029866218567\n",
            "Epoch: 2; Loss: 1.8069299459457397\n",
            "Epoch: 2; Loss: 1.7377585172653198\n",
            "Epoch: 2; Loss: 1.636670708656311\n",
            "Epoch: 2; Loss: 1.7596352100372314\n",
            "Epoch: 2; Loss: 1.842854619026184\n",
            "Epoch: 2; Loss: 2.1599831581115723\n",
            "Epoch: 2; Loss: 1.580687403678894\n",
            "Epoch: 2; Loss: 1.6735213994979858\n",
            "Epoch: 2; Loss: 1.7383989095687866\n",
            "Epoch: 2; Loss: 1.754809856414795\n",
            "Epoch: 2; Loss: 1.6275651454925537\n",
            "Epoch: 2; Loss: 1.6750617027282715\n",
            "Epoch: 2; Loss: 1.5942546129226685\n",
            "Epoch: 2; Loss: 1.5137680768966675\n",
            "Epoch: 2; Loss: 1.8683372735977173\n",
            "Epoch: 2; Loss: 1.8782176971435547\n",
            "Epoch: 2; Loss: 1.6613553762435913\n",
            "Epoch: 2; Loss: 1.6217001676559448\n",
            "Epoch: 2; Loss: 1.8665534257888794\n",
            "Epoch: 2; Loss: 1.794959545135498\n",
            "Epoch: 2; Loss: 1.8381717205047607\n",
            "Epoch: 2; Loss: 1.6532180309295654\n",
            "Epoch: 2; Loss: 1.6441179513931274\n",
            "Epoch: 2; Loss: 1.7304242849349976\n",
            "Epoch: 2; Loss: 1.784919023513794\n",
            "Epoch: 2; Loss: 1.8041319847106934\n",
            "Epoch: 2; Loss: 1.594565749168396\n",
            "Epoch: 2; Loss: 1.7482812404632568\n",
            "Epoch: 2; Loss: 1.692939043045044\n",
            "Epoch: 2; Loss: 1.5804921388626099\n",
            "Epoch: 2; Loss: 1.5714114904403687\n",
            "Epoch: 2; Loss: 1.611217737197876\n",
            "Epoch: 2; Loss: 1.891947865486145\n",
            "Epoch: 2; Loss: 1.6484389305114746\n",
            "Epoch: 2; Loss: 1.7805720567703247\n",
            "Epoch: 2; Loss: 1.7509561777114868\n",
            "Epoch: 2; Loss: 2.017228126525879\n",
            "Epoch: 2; Loss: 1.436152696609497\n",
            "Epoch: 2; Loss: 1.5840201377868652\n",
            "Epoch: 2; Loss: 1.5006484985351562\n",
            "Epoch: 2; Loss: 1.6264568567276\n",
            "Epoch: 2; Loss: 1.5987259149551392\n",
            "Epoch: 2; Loss: 1.8423000574111938\n",
            "Epoch: 2; Loss: 2.0191142559051514\n",
            "Epoch: 2; Loss: 1.628378987312317\n",
            "Epoch: 2; Loss: 1.7309576272964478\n",
            "Epoch: 2; Loss: 1.906042218208313\n",
            "Epoch: 2; Loss: 1.8030246496200562\n",
            "Epoch: 2; Loss: 1.7452137470245361\n",
            "Epoch: 2; Loss: 1.739001989364624\n",
            "Epoch: 2; Loss: 1.620383620262146\n",
            "Epoch: 2; Loss: 1.8245549201965332\n",
            "Epoch: 2; Loss: 1.739508867263794\n",
            "Epoch: 2; Loss: 1.6315977573394775\n",
            "Epoch: 2; Loss: 1.7585105895996094\n",
            "Epoch: 2; Loss: 1.4504064321517944\n",
            "Epoch: 2; Loss: 1.657278299331665\n",
            "Epoch: 2; Loss: 1.5156776905059814\n",
            "Epoch: 2; Loss: 1.879587173461914\n",
            "Epoch: 2; Loss: 1.6469374895095825\n",
            "Epoch: 2; Loss: 1.696489930152893\n",
            "Epoch: 2; Loss: 1.6344014406204224\n",
            "Epoch: 2; Loss: 1.6434292793273926\n",
            "Epoch: 2; Loss: 1.882511854171753\n",
            "Epoch: 2; Loss: 1.620382308959961\n",
            "Epoch: 2; Loss: 1.877305030822754\n",
            "Epoch: 2; Loss: 1.6766400337219238\n",
            "Epoch: 2; Loss: 1.9894596338272095\n",
            "Epoch: 2; Loss: 1.7243022918701172\n",
            "Epoch: 2; Loss: 1.7251399755477905\n",
            "Epoch: 2; Loss: 1.642001748085022\n",
            "Epoch: 2; Loss: 1.6940490007400513\n",
            "Epoch: 2; Loss: 1.7852977514266968\n",
            "Epoch: 2; Loss: 1.4214160442352295\n",
            "Epoch: 2; Loss: 1.658141016960144\n"
          ]
        }
      ],
      "source": [
        "# optimizer = torch.optim.AdamW(\n",
        "#     model.parameters(),\n",
        "#     lr = 5e-5,\n",
        "#     eps = 1e-08\n",
        "# )\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), eps = 1e-08, lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    for batch in train_loader:\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}; Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhL5AHbr4QbC"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation\n",
        "        for batch in val_loader:\n",
        "            # Move data to the same device as the model\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Get model outputs\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Predicted logits\n",
        "\n",
        "            # Get the predicted class (highest score)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    acc = accuracy_score(true_labels, predictions)\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predictions, target_names=['pants-on-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(true_labels, predictions))\n",
        "\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ9AP7Uf4QbC"
      },
      "outputs": [],
      "source": [
        "val_accuracy = evaluate_model(model, val_loader, device)\n",
        "test_accuracy = evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m-bDkVu4QbC"
      },
      "outputs": [],
      "source": [
        "# bert-base-uncased\n",
        "# Fine Tune\n",
        "# epochs = 5\n",
        "# optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), eps = 1e-08, lr=1e-5)\n",
        "# batch_size = 16\n",
        "\n",
        "# Validation\n",
        "# Accuracy: 0.2679\n",
        "# Classification Report:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "# pants-on-fire       0.38      0.20      0.26       116\n",
        "#         false       0.27      0.32      0.30       263\n",
        "#   barely-true       0.24      0.14      0.17       237\n",
        "#     half-true       0.31      0.27      0.29       248\n",
        "#   mostly-true       0.28      0.33      0.31       251\n",
        "#          true       0.20      0.33      0.25       169\n",
        "\n",
        "#      accuracy                           0.27      1284\n",
        "#     macro avg       0.28      0.26      0.26      1284\n",
        "#  weighted avg       0.28      0.27      0.26      1284\n",
        "\n",
        "\n",
        "# Confusion Matrix:\n",
        "# [[23 37 11 16  9 20]\n",
        "#  [13 84 35 38 48 45]\n",
        "#  [10 72 32 36 43 44]\n",
        "#  [ 8 50 27 67 53 43]\n",
        "#  [ 4 35 24 39 83 66]\n",
        "#  [ 3 28  3 23 57 55]]\n",
        "\n",
        "# Testing\n",
        "# Accuracy: 0.2676\n",
        "# Classification Report:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "# pants-on-fire       0.34      0.13      0.19        92\n",
        "#         false       0.28      0.32      0.30       249\n",
        "#   barely-true       0.25      0.14      0.18       212\n",
        "#     half-true       0.27      0.22      0.24       265\n",
        "#   mostly-true       0.24      0.34      0.28       241\n",
        "#          true       0.29      0.38      0.33       208\n",
        "\n",
        "#      accuracy                           0.27      1267\n",
        "#     macro avg       0.28      0.25      0.25      1267\n",
        "#  weighted avg       0.27      0.27      0.26      1267\n",
        "\n",
        "\n",
        "# Confusion Matrix:\n",
        "# [[12 30  9 10 17 14]\n",
        "#  [ 6 79 26 37 52 49]\n",
        "#  [ 7 48 30 48 55 24]\n",
        "#  [ 6 60 25 57 79 38]\n",
        "#  [ 1 34 17 37 83 69]\n",
        "#  [ 3 31 12 22 62 78]]\n",
        "\n",
        "\n",
        "# bert-large-uncased\n",
        "# Fine Tunestate\n",
        "# epochs = 5\n",
        "# optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), eps = 1e-08, lr=1e-5)\n",
        "# batch_size = 16\n",
        "\n",
        "# Validation\n",
        "\n",
        "# Accuracy: 0.2757\n",
        "# Classification Report:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "# pants-on-fire       0.35      0.15      0.21       116\n",
        "#         false       0.32      0.33      0.33       263\n",
        "#   barely-true       0.27      0.19      0.22       237\n",
        "#     half-true       0.24      0.29      0.26       248\n",
        "#   mostly-true       0.29      0.33      0.31       251\n",
        "#          true       0.23      0.31      0.27       169\n",
        "\n",
        "#      accuracy                           0.28      1284\n",
        "#     macro avg       0.28      0.26      0.27      1284\n",
        "#  weighted avg       0.28      0.28      0.27      1284\n",
        "\n",
        "\n",
        "# Confusion Matrix:\n",
        "# [[17 36 19 16 15 13]\n",
        "#  [14 87 44 44 33 41]\n",
        "#  [ 5 52 44 66 35 35]\n",
        "#  [ 5 39 33 71 64 36]\n",
        "#  [ 6 34 16 63 82 50]\n",
        "#  [ 2 21  8 32 53 53]]\n",
        "\n",
        "# Testing\n",
        "\n",
        "# Accuracy: 0.2636\n",
        "# Classification Report:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "# pants-on-fire       0.40      0.18      0.25        92\n",
        "#         false       0.28      0.27      0.27       249\n",
        "#   barely-true       0.24      0.16      0.19       212\n",
        "#     half-true       0.25      0.26      0.26       265\n",
        "#   mostly-true       0.26      0.34      0.29       241\n",
        "#          true       0.27      0.31      0.29       208\n",
        "\n",
        "#      accuracy                           0.26      1267\n",
        "#     macro avg       0.28      0.25      0.26      1267\n",
        "#  weighted avg       0.27      0.26      0.26      1267\n",
        "\n",
        "\n",
        "# Confusion Matrix:\n",
        "# [[17 31 12  9 16  7]\n",
        "#  [ 9 67 28 52 43 50]\n",
        "#  [ 2 45 33 63 44 25]\n",
        "#  [ 6 46 37 70 65 41]\n",
        "#  [ 3 28 16 60 83 51]\n",
        "#  [ 5 26 14 28 71 64]]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
